\section{Conclusion and Lessons Learned}
Over all we learnt a lot during this project. Especially, which problems arise if you work with big datasets. Or let's correct that, for our experience, enormous ones with 80 GB and about 200 million entries.
Multiple times we had to take a step back and rethink our approach. To handle the data correctly and not lose to much time.
As a consequence steps like cleaning and testing the correctness of the integration suffered under such constraints.

For a future project, where we could select data ourself, we definitely would select smaller datasets. Not small ones, but smaller, more around 10 to 20 GB to still have the challenge but give a bit more space for the time management.
Further, we would change the way on how to approach different parts of the whole process.
Mainly to write a fully-fledged generalized interface on how to clean data and communicate with the database can help enourmous in the long run. We kind of did that already but the extra mile in the beginning helps a lot throughout the whole process.
Also creating a small sample set of the used datasets before starting can speed up the development process and help to evaluate the correctness of the integrated data.


